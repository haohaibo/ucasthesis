%---------------------------------------------------------------------------%
%->> 封面信息及生成
%---------------------------------------------------------------------------%
%-
%-> 中文封面信息
%-
\confidential{}% 密级：只有涉密论文才填写
\schoollogo{scale=0.095}{ucas_logo}% 校徽
\title{面向GPU体系结构的通用矩阵乘优化研究}% 论文中文题目
\author{郝海波}% 论文作者
%\advisor{马捷~正研级高工~中国科学院计算技术研究所}% 指导教师：姓名 专业技术职务 工作单位
\advisor{马捷~正研级高工}% 指导教师：姓名 专业技术职务 工作单位
\advisorsec{中国科学院计算技术研究所}% 指导老师附加信息 或 第二指导老师信息
\degree{硕士}% 学位：学士、硕士、博士
\degreetype{工程}% 学位类别：理学、工学、工程、医学等
\major{计算机技术}% 二级学科专业名称
\institute{中国科学院计算技术研究所}% 院系名称
\chinesedate{2018~年~5~月}% 毕业日期：夏季为6月、冬季为12月
%-
%-> 英文封面信息
%-
\englishtitle{General Matrix Multiplication optimization for\\ GPU architecture}% 论文英文题目
\englishauthor{Hao Haibo}% 论文作者
\englishadvisor{Supervisor: Professor Ma Jie}% 指导教师
\englishdegree{Master}% 学位：Bachelor, Master, Doctor。封面格式将根据英文学位名称自动切换，请确保拼写准确无误
\englishdegreetype{Engineering}% 学位类别：Philosophy, Natural Science, Engineering, Economics, Agriculture 等
\englishthesistype{thesis}% 论文类型： thesis, dissertation
\englishmajor{Computer Technology}% 二级学科专业名称
\englishinstitute{Institute of Computing Technology\\ Chinese Academy of Sciences}% 院系名称
\englishdate{May, 2018}% 毕业日期：夏季为June、冬季为December
%-
%-> 生成封面
%-
\maketitle% 生成中文封面
\makeenglishtitle% 生成英文封面
%-
%-> 作者声明
%-
\makedeclaration% 生成声明页
%-
%-> 中文摘要
%-
\chapter*{摘\quad 要}\chaptermark{摘\quad 要}% 摘要标题
\setcounter{page}{1}% 开始页码
\pagenumbering{Roman}% 页码符号

当前，一颗GPU芯片上集成的核心数越来越多，GPU体系结构也在非常快速的演变。目前主要的高端GPU芯片厂商有NVIDIA和AMD。主流的GPUs架构有NVIDIA Kepler，Maxwell，Pascal和Volta GPU；AMD Fiji，Vega10，Vega20 GPU。由于每一代的GPU架构都会发生变化，我们就需要在新的架构上重新做优化工作。不幸的是，我们没有可用的性能上界分析方法和工具。在实际中，开发人员通过算法的分析和积累的经验，采用多种优化手段来编写高效的kernel。Kernel编写人员可能会通过性能分析工具(如NVVP\citepns{profiler2011nvidia})的分析结果来指导进一步的优化。然而，这样并不能知道现在优化的结果距离性能上界还有多远。

实现GPU (Graphic Processing Units)上的快速通用矩阵乘一直以来都是自动调优工具和kernel编写人员所追求的目标。在本篇文章中，我们尝试提供一种GPUs上算法性能上界的方法，并做出汇编级的基准测试。Meng等人\citepns{meng2011grophecy}提出了GPU性能预测框架，该框架基于标注代码框架。Hong和Kim提出了MWP-CWP\citepns{hong2009analytical}模型来预测CUDA应用程序性能，该模型基于NVIDIA PTX。Sim\citepns{sim2012performance}等人在2012年将MWP-CWP模型进行了扩展，使用汇编编写kernel预测程序性能。Zhang和Owen\citepns{zhang2011quantitative}基于汇编程序提出了GPU定量分析模型。由于关于GPU微架构的资料非常少，我们无法针对新一代GPU架构做出精确的GPU模拟器。但我们通过上面这些分析方法可以十分近似的预测GPU程序的性能。Roofline\citepns{williams2009roofline}模型是应用最广的用来评估优化效果的模型，本文将采用该模型对SGEMM做性能分析。

对于AMD GPU，其官方并没有提供良好的性能分析工具。但AMD GPU有可用的LLVM汇编器。本文通过手工汇编优化的手法，在Fiji 和Vega GPU上实现的矩阵乘性能达到95\%。


\keywords{矩阵乘，AMD GPU，汇编，延迟掩盖，性能分析}% 中文关键词
%-
%-> 英文摘要
%-
\chapter*{Abstract}\chaptermark{Abstract}% 摘要标题

Currently, the number of cores integrated on a GPU chip is increasing, and the GPU architecture is also rapidly evolving. At present, the main high-end GPU chip manufacturers are NVIDIA and AMD. Mainstream GPUs include NVIDIA Kepler, Maxwell, Pascal, and Volta GPUs; AMD Fiji, Vega10, and Vega20 GPUs. As every generation of GPU architecture changes, we need to re-optimize the new architecture. Unfortunately, we do not have available methods and tools for analyzing upper bounds of performance. In practice, developers use a variety of optimization methods to write efficient kernels through algorithm analysis and accumulated experience. Kernel writers may direct further optimization through the analysis of performance analysis tools such as NVVP\citepns{profiler2011nvidia}. However, it does not know how far the optimization results are now far from the upper bound of performance.

Fast General Matrix Multiplication on GPUs (Graphic Processing Units) has long been a goal pursued by auto-tuning tools and kernel writers. In this article, we try to provide a method for upper bounds of algorithm performance on GPUs and make assembly level benchmarks. Meng et al.\citepns{meng2011grophecy} proposed a GPU performance prediction framework based on an annotation code framework. Hong and Kim proposed the MWP-CWP\citepns{hong2009analytical} model to predict CUDA application performance based on NVIDIA PTX. Sim et al.\citepns{sim2012performance} extended the MWP-CWP model in 2012 and used assembly to write the kernel to predict program performance. Zhang and Owens\citepns{zhang2011quantitative} proposed a GPU quantitative analysis model based on assembler. Since there is very little information about the GPU microarchitecture, we cannot make an accurate GPU emulator for the next-generation GPU architecture. However, we can predict the performance of GPU programs very similarly through the above analysis methods. The Roofline\citepns{williams2009roofline} model is the most widely used model for evaluating the optimization effect. This paper will use this model to perform performance analysis on SGEMM.

For the AMD GPU, the official does not provide a good performance analysis tools. However, the LLVM assembler is available for AMD GPUs. In this paper, the matrix multiplication performance achieved on the Fiji and Vega GPUs reaches 95\% through manual assembly optimization.


\englishkeywords{GEMM, AMD GPU, assembly, letency hiding, performance analysis}% 英文关键词
%---------------------------------------------------------------------------%
